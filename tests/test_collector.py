"""
test_collector.py
Tests unitaires pour le module collector.py
"""

import unittest
import pandas as pd
import os
import sys
import tempfile
import shutil
from datetime import datetime, timedelta
import yaml

# Ajoute le r√©pertoire src au path Python
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from collector import RedditCollector

class TestRedditCollector(unittest.TestCase):
    """Tests pour la classe RedditCollector"""
    
    def setUp(self):
        """Setup avant chaque test"""
        # Cr√©e un dossier temporaire pour les tests
        self.test_dir = tempfile.mkdtemp()
        self.backup_dir = os.path.join(self.test_dir, 'data', 'backup')
        os.makedirs(self.backup_dir, exist_ok=True)
        
        # Cr√©e un dataset de test minimal
        self.create_test_dataset()
        
        # Cr√©e un fichier config.yaml de test
        self.create_test_config()
        
        # Initialise le collecteur avec le dossier de test
        os.environ['TEST_MODE'] = 'True'
    
    def tearDown(self):
        """Nettoyage apr√®s chaque test"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
        if 'TEST_MODE' in os.environ:
            del os.environ['TEST_MODE']
    
    def create_test_dataset(self):
        """Cr√©e un dataset de test pour les tests unitaires"""
        # Dataset minimal de 50 posts
        data = []
        services = ['AWS', 'Azure', 'GitHub']
        
        for i in range(50):
            is_outage = i % 10 == 0  # 10% d'anomalies (5 posts)
            service = services[i % 3]
            
            data.append({
                'id': f'test_post_{i:03d}',
                'title': f"{'URGENT: ' if is_outage else ''}{service} service {'outage' if is_outage else 'discussion'}",
                'service': service,
                'urgency': 'HIGH' if is_outage else 'LOW',
                'is_outage': 1 if is_outage else 0,
                'created_datetime': (datetime.now() - timedelta(hours=i)).isoformat(),
                'score': 100 if is_outage else 20,
                'num_comments': 50 if is_outage else 5,
                'incident_id': f'INC-{i//10:03d}' if is_outage else None,
                'source': 'test_dataset'
            })
        
        df = pd.DataFrame(data)
        
        # Sauvegarde le dataset de test
        test_file = os.path.join(self.backup_dir, 'test_dataset.csv')
        df.to_csv(test_file, index=False)
        
        # Cr√©e aussi un fichier avec le pattern attendu
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        analysis_file = os.path.join(self.backup_dir, f'reddit_cloud_dataset_analysis_{timestamp}.csv')
        df.to_csv(analysis_file, index=False)
        
        self.test_dataset_path = test_file
        self.analysis_dataset_path = analysis_file
    
    def create_test_config(self):
        """Cr√©e un fichier config.yaml de test"""
        config_content = """
app:
  subreddits: ["aws", "azure", "github"]
  keywords: ["down", "outage", "error", "slow"]
  check_interval: 60
  history_days: 7

data:
  backup_dir: "data/backup"
  output_dir: "data/live"

reddit:
  client_id: "test_client_id"
  client_secret: "test_client_secret"
  user_agent: "TestAgent/1.0"
"""
        
        config_path = os.path.join(self.test_dir, 'config.yaml')
        with open(config_path, 'w') as f:
            f.write(config_content)
        
        # Modifie le collecteur pour utiliser ce config
        original_load_config = RedditCollector.load_config
        
        def mock_load_config(self):
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            # Modifie le backup_dir pour pointer vers notre dossier de test
            config['data']['backup_dir'] = self.backup_dir
            return config
        
        self.original_load_config = original_load_config
        RedditCollector.load_config = mock_load_config
    
    def test_01_collector_initialization(self):
        """Test l'initialisation du collecteur"""
        print("\nüß™ Test 1: Initialisation du collecteur")
        
        # Test sans API
        collector = RedditCollector(use_api=False)
        self.assertIsNotNone(collector)
        self.assertFalse(collector.use_api)
        self.assertIsNone(collector.reddit_client)
        
        # Test avec API (mais pas de vrai client)
        collector_with_api = RedditCollector(use_api=True)
        self.assertIsNotNone(collector_with_api)
        self.assertTrue(collector_with_api.use_api)
        
        print("‚úÖ Initialisation r√©ussie")
    
    def test_02_config_loading(self):
        """Test le chargement de la configuration"""
        print("\nüß™ Test 2: Chargement de la configuration")
        
        collector = RedditCollector(use_api=False)
        
        # V√©rifie que la config est charg√©e
        self.assertIsInstance(collector.config, dict)
        self.assertIn('app', collector.config)
        self.assertIn('data', collector.config)
        
        # V√©rifie les valeurs sp√©cifiques
        self.assertEqual(len(collector.config['app']['subreddits']), 3)
        self.assertIn('aws', collector.config['app']['subreddits'])
        
        print(f"‚úÖ Configuration charg√©e: {len(collector.config['app']['subreddits'])} subreddits")
    
    def test_03_load_backup_dataset(self):
        """Test le chargement du dataset backup"""
        print("\nüß™ Test 3: Chargement du dataset backup")
        
        collector = RedditCollector(use_api=False)
        df = collector.load_latest_backup_dataset()
        
        # V√©rifie que le DataFrame n'est pas vide
        self.assertIsNotNone(df)
        self.assertFalse(df.empty)
        
        # V√©rifie le nombre de posts
        self.assertEqual(len(df), 50)
        
        # V√©rifie les colonnes essentielles
        required_columns = ['id', 'title', 'service', 'urgency', 'is_outage']
        for col in required_columns:
            self.assertIn(col, df.columns)
        
        # V√©rifie les statistiques
        anomalies = df['is_outage'].sum()
        self.assertEqual(anomalies, 5)  # 10% de 50 = 5 anomalies
        
        print(f"‚úÖ Dataset charg√©: {len(df)} posts, {anomalies} anomalies")
    
    def test_04_urgency_analysis(self):
        """Test l'analyse d'urgence"""
        print("\nüß™ Test 4: Analyse d'urgence")
        
        collector = RedditCollector(use_api=False)
        
        # Test avec diff√©rents textes
        test_cases = [
            ("CRITICAL: AWS EC2 down in us-east-1", "CRITICAL"),
            ("Major outage on Azure services", "HIGH"),
            ("Performance issues with GitHub Actions", "MEDIUM"),
            ("Discussion about cloud best practices", "LOW"),
            ("Help needed with AWS configuration", "LOW")
        ]
        
        for text, expected_urgency in test_cases:
            urgency = collector.analyze_urgency(text)
            self.assertEqual(urgency, expected_urgency)
        
        print("‚úÖ Analyse d'urgence fonctionnelle")
    
    def test_05_service_extraction(self):
        """Test l'extraction de service cloud"""
        print("\nüß™ Test 5: Extraction de service cloud")
        
        collector = RedditCollector(use_api=False)
        
        test_cases = [
            ("AWS EC2 instance failing", "AWS"),
            ("Azure Blob Storage issues", "Azure"),
            ("GitHub Actions not working", "GitHub"),
            ("Google Cloud Platform outage", "Google Cloud"),
            ("DigitalOcean droplet problems", "DigitalOcean"),
            ("General discussion about cloud", "Unknown")
        ]
        
        for text, expected_service in test_cases:
            service = collector.extract_cloud_service(text)
            self.assertEqual(service, expected_service)
        
        print("‚úÖ Extraction de service fonctionnelle")
    
    def test_06_outage_detection(self):
        """Test la d√©tection de pannes"""
        print("\nüß™ Test 6: D√©tection de pannes")
        
        collector = RedditCollector(use_api=False)
        
        test_cases = [
            ("AWS is down right now", 1),
            ("Major outage reported", 1),
            ("Service not working", 1),
            ("Performance discussion", 0),
            ("Best practices guide", 0),
            ("How to optimize costs", 0)
        ]
        
        for text, expected_is_outage in test_cases:
            is_outage = collector.is_outage_post(text)
            self.assertEqual(is_outage, expected_is_outage)
        
        print("‚úÖ D√©tection de pannes fonctionnelle")
    
    def test_07_simulate_real_time(self):
        """Test la simulation temps r√©el"""
        print("\nüß™ Test 7: Simulation temps r√©el")
        
        collector = RedditCollector(use_api=False)
        df = collector.load_latest_backup_dataset()
        
        # Test la simulation
        realtime_df = collector.simulate_real_time_collection(df, hours_back=24)
        
        # V√©rifie que le r√©sultat n'est pas vide
        self.assertFalse(realtime_df.empty)
        
        # V√©rifie que c'est un sous-ensemble du dataset original
        self.assertLessEqual(len(realtime_df), len(df))
        
        print(f"‚úÖ Simulation temps r√©el: {len(realtime_df)} posts r√©cents")
    
    def test_08_collect_data_method(self):
        """Test la m√©thode principale collect_data"""
        print("\nüß™ Test 8: M√©thode collect_data")
        
        collector = RedditCollector(use_api=False)
        
        # Test en mode backup
        df = collector.collect_data(mode='backup', simulate_realtime=False)
        
        # V√©rifications
        self.assertFalse(df.empty)
        self.assertEqual(len(df), 50)
        self.assertIn('source', df.columns)
        
        # V√©rifie les colonnes pr√©sentes
        expected_cols = ['id', 'title', 'service', 'urgency', 'is_outage']
        for col in expected_cols:
            self.assertIn(col, df.columns)
        
        print(f"‚úÖ collect_data r√©ussie: {len(df)} posts collect√©s")
    
    def test_09_save_collected_data(self):
        """Test la sauvegarde des donn√©es collect√©es"""
        print("\nüß™ Test 9: Sauvegarde des donn√©es")
        
        collector = RedditCollector(use_api=False)
        df = collector.collect_data(mode='backup', simulate_realtime=False)
        
        # Sauvegarde les donn√©es
        output_dir = os.path.join(self.test_dir, 'data', 'live')
        collector.config['data']['output_dir'] = output_dir
        
        collector.save_collected_data(df, output_type='both')
        
        # V√©rifie que les fichiers ont √©t√© cr√©√©s
        raw_files = [f for f in os.listdir(os.path.join(output_dir, 'raw')) if f.endswith('.csv')]
        processed_files = [f for f in os.listdir(os.path.join(output_dir, 'processed')) if f.endswith('.csv')]
        
        self.assertGreater(len(raw_files), 0)
        self.assertGreater(len(processed_files), 0)
        
        # V√©rifie le contenu des fichiers
        raw_file_path = os.path.join(output_dir, 'raw', raw_files[0])
        raw_df = pd.read_csv(raw_file_path)
        self.assertEqual(len(raw_df), len(df))
        
        print(f"‚úÖ Donn√©es sauvegard√©es: {len(raw_files)} fichier(s) raw, {len(processed_files)} fichier(s) processed")
    
    def test_10_minimal_dataset_fallback(self):
        """Test le fallback sur dataset minimal"""
        print("\nüß™ Test 10: Fallback dataset minimal")
        
        # Supprime les fichiers de dataset pour forcer le fallback
        for f in os.listdir(self.backup_dir):
            os.remove(os.path.join(self.backup_dir, f))
        
        collector = RedditCollector(use_api=False)
        
        # Cette m√©thode devrait cr√©er un dataset minimal
        df = collector.load_latest_backup_dataset()
        
        # V√©rifications
        self.assertFalse(df.empty)
        self.assertGreater(len(df), 0)
        self.assertIn('source', df.columns)
        self.assertTrue(all(df['source'] == 'minimal_backup'))
        
        print(f"‚úÖ Fallback r√©ussi: {len(df)} posts cr√©√©s")
    
    def test_11_statistics_generation(self):
        """Test la g√©n√©ration de statistiques"""
        print("\nüß™ Test 11: G√©n√©ration de statistiques")
        
        collector = RedditCollector(use_api=False)
        df = collector.collect_data(mode='backup', simulate_realtime=False)
        
        # V√©rifie que les statistiques de base sont correctes
        total_posts = len(df)
        anomalies = df['is_outage'].sum() if 'is_outage' in df.columns else 0
        
        self.assertEqual(total_posts, 50)
        self.assertEqual(anomalies, 5)  # 10% de 50
        
        # V√©rifie la distribution des services
        if 'service' in df.columns:
            service_counts = df['service'].value_counts()
            self.assertIn('AWS', service_counts.index)
            self.assertIn('Azure', service_counts.index)
            self.assertIn('GitHub', service_counts.index)
        
        print(f"‚úÖ Statistiques: {total_posts} posts, {anomalies} anomalies")
    
    def test_12_integration_with_pandas(self):
        """Test l'int√©gration avec pandas"""
        print("\nüß™ Test 12: Int√©gration pandas")
        
        # Charge le dataset directement avec pandas
        df = pd.read_csv(self.analysis_dataset_path)
        
        # V√©rifie les types de donn√©es
        self.assertIsInstance(df, pd.DataFrame)
        
        # Op√©rations pandas basiques
        filtered = df[df['is_outage'] == 1]
        grouped = df.groupby('service')['score'].mean()
        
        self.assertEqual(len(filtered), 5)  # 5 anomalies
        self.assertGreater(len(grouped), 0)
        
        print("‚úÖ Int√©gration pandas r√©ussie")
    
    def test_13_data_quality_checks(self):
        """Test les v√©rifications de qualit√© des donn√©es"""
        print("\nüß™ Test 13: Qualit√© des donn√©es")
        
        df = pd.read_csv(self.analysis_dataset_path)
        
        # V√©rifie l'absence de valeurs nulles dans les colonnes critiques
        critical_columns = ['id', 'title', 'service', 'is_outage']
        
        for col in critical_columns:
            if col in df.columns:
                null_count = df[col].isnull().sum()
                self.assertEqual(null_count, 0, f"Colonne {col} a {null_count} valeurs nulles")
        
        # V√©rifie les types de donn√©es
        if 'is_outage' in df.columns:
            self.assertTrue(pd.api.types.is_numeric_dtype(df['is_outage']))
        
        if 'score' in df.columns:
            self.assertTrue(pd.api.types.is_numeric_dtype(df['score']))
        
        print("‚úÖ Qualit√© des donn√©es valid√©e")

def run_all_tests():
    """Ex√©cute tous les tests et g√©n√®re un rapport"""
    print("="*70)
    print("üß™ SUITE DE TESTS - REDDIT COLLECTOR")
    print("="*70)
    
    # Cr√©e un test suite
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestRedditCollector)
    
    # Ex√©cute les tests avec un runner personnalis√©
    runner = unittest.TextTestRunner(verbosity=2, descriptions=True)
    
    print("\n‚ñ∂Ô∏è  D√©marrage des tests...\n")
    result = runner.run(suite)
    
    # R√©sum√©
    print("\n" + "="*70)
    print("üìä RAPPORT DES TESTS")
    print("="*70)
    
    total = result.testsRun
    passed = total - len(result.failures) - len(result.errors)
    
    print(f"Total tests: {total}")
    print(f"‚úÖ Tests r√©ussis: {passed}")
    print(f"‚ùå Tests √©chou√©s: {len(result.failures)}")
    print(f"‚ö†Ô∏è  Tests avec erreurs: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("\nüéâ TOUS LES TESTS ONT R√âUSSI !")
        print("Le collecteur est pr√™t pour la production.")
    else:
        print("\nüîß Certains tests n√©cessitent des corrections.")
    
    print("="*70)
    
    return result.wasSuccessful()

if __name__ == "__main__":
    # Ex√©cute tous les tests
    success = run_all_tests()
    
    # Code de sortie pour CI/CD
    sys.exit(0 if success else 1)